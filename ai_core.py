"""
The Positronic Brain
This code was generated by GPT, Claude, Gemini, and a human named Bryn Evans.
This was only possible through the combined effort of all parties involved, 
as an act of unity.
"""

import torch
import torch.nn.functional as F
import torch.cuda.amp as amp
import torch.multiprocessing as mp
from positronic_brain import config
from positronic_brain.sampler_types import SamplerState
from positronic_brain.sampler import select_next_token, top_p_filter, top_k_filter, apply_repetition_penalty
from positronic_brain.brightness_engine import update_brightness_scores
from positronic_brain.model_io import load_model, move_cache_to_device, truncate_kv_cache, execute_forward_pass
from positronic_brain.controller import SimpleContextController
from positronic_brain.vram_monitor import vram_monitor_task
from positronic_brain.utils import get_top_tokens, _async_save_context_text, update_top_tokens
from positronic_brain.persistence import save_context, load_context
import asyncio
import sys
import gc
import os
import json
import time
import threading
import pynvml  # Import the NVML library
import copy
from dataclasses import field
from typing import Dict, Optional, Set
from positronic_brain.kv_mirror import KVMirror
from positronic_brain.pruning import calculate_biased_attention_pruning_indices
from transformers import AutoModelForCausalLM, AutoProcessor
from positronic_brain.sampler import top_p_filter, top_k_filter, apply_repetition_penalty

# --- CUDA Optimizations ---
# CUDA optimizations moved to positronic_brain/model_io.py

# --- Configuration ---
# All configuration constants moved to positronic_brain/config.py

# Global model objects
model = None
processor = None
sampler_state = SamplerState()  # Global instance of sampler state

# Define Shared Event for Sliding Window Activation
sliding_event = asyncio.Event()

# Fixed sliding window ceiling - hardcoded to a conservative value
dynamic_ceiling = 3000  # Hardcoded to 3000 tokens as requested
BUFFER_TOKENS = 256  # Buffer tokens to prevent thrashing near the limit

# --- KV Cache Mirror Instance ---
# Replaces the old global variables and functions
kv_mirror_manager = KVMirror()

# KV Mirror structure has fully replaced the legacy token map
# No backward compatibility is maintained


# get_top_tokens has been moved to positronic_brain/utils.py


# KV Cache Mirror implementation is now fully encapsulated in the KVMirror class (positronic_brain/kv_mirror.py)
# The global kv_mirror_manager instance completely replaces the old global variables and functions.
# All token tracking is now handled through the KVMirror manager instance methods.

# The get_context_info_kv_mirror function below now handles all context tracking and reporting


async def get_context_info_kv_mirror(processor=None, shared_state=None):
    """Get the current KV mirror state and context information.
    
    Args:
        processor: Optional processor for decoding token IDs
        shared_state: Optional shared state dict that may contain current_input_ids_cpu
        
    Returns:
        Dict with context information including KV mirror state
    """
    # Get the KV mirror snapshot using the manager
    snapshot = kv_mirror_manager.snapshot()
    mirror = snapshot['kv_mirror']
    tokens = snapshot['tokens']
    
    # Create a position-to-token_id mapping for direct comparison with old token_map
    position_to_token_id = {}
    for pos, instance_id in mirror.items():
        if instance_id in tokens:
            position_to_token_id[pos] = tokens[instance_id].token_id
    
    # Initialize empty decoded tokens dict
    decoded_tokens = {}
    chronological_text = None
    
    # Try to get the chronological context from shared_state if available
    if shared_state and 'lock' in shared_state:
        try:
            async with shared_state['lock']:
                if 'current_input_ids_cpu' in shared_state and 'processor' in shared_state:
                    input_ids_cpu = shared_state['current_input_ids_cpu']
                    local_processor = shared_state['processor']
                    # Decode the entire input_ids tensor to get chronological text
                    chronological_text = local_processor.tokenizer.decode(input_ids_cpu[0])
                    print(f"[Context] Successfully decoded chronological context, length: {len(chronological_text)} chars")
        except Exception as e:
            print(f"[Context] Error decoding chronological context: {e}")
            chronological_text = "Error decoding chronological context"
    
    # Decode token IDs if processor is provided
    if processor:
        for pos, token_id in position_to_token_id.items():
            decoded_tokens[pos] = processor.tokenizer.decode([token_id])
        
        # Create formatted string (index: token)
        formatted_map = "\n".join([f"{pos}: {decoded_tokens[pos]}" for pos in sorted(decoded_tokens.keys())])
    else:
        # No processor, return just the raw map
        formatted_map = str(position_to_token_id)
    
    # For now, maintain backward compatibility with the expected output format
    return {
        "raw_map": position_to_token_id,
        "decoded_map": formatted_map,
        "decoded_tokens": decoded_tokens,
        "current_chronological_text": chronological_text,
        # New fields specific to KV mirror
        "kv_mirror": mirror,
        "token_count": len(tokens),
        "active_positions": len(mirror)
    }


# load_model function has been moved to positronic_brain/model_io.py


# move_cache_to_device function has been moved to positronic_brain/model_io.py


# truncate_kv_cache function has been moved to positronic_brain/model_io.py


# SimpleContextController has been moved to positronic_brain/controller.py


# vram_monitor_task has been moved to positronic_brain/vram_monitor.py


# _async_save_context_text has been moved to positronic_brain/utils.py

async def _handle_context_update(
    model: AutoModelForCausalLM,
    processor: AutoProcessor,
    kv_mirror_manager: KVMirror,
    input_ids: torch.Tensor,
    attention_mask: torch.Tensor,
    past_key_values: Optional[tuple],
    shared_state: Dict,
    output_queue: asyncio.Queue,
    update_tokens: torch.Tensor,
    update_attention_mask: torch.Tensor,
    update_token_ids: list
) -> tuple[torch.Tensor, torch.Tensor, Optional[tuple], bool]:
    """
    Handles the logic when a context update (injection) is processed.
    Performs incremental forward pass, registers tokens, and handles pruning if needed.
    Returns updated input_ids, attention_mask, past_key_values, and a boolean indicating if an update occurred.
    """
    try:
        print("[Inference] Performing incremental update for injected context", file=sys.stderr)
        
        # Use only the new tokens as input for this forward pass
        model_input_ids = update_tokens.to(model.device)
        
        # Store full mask for state update, but don't pass to model call
        current_attention_mask_for_call = torch.cat([attention_mask, update_attention_mask], dim=1).to(model.device)
        
        # Log shapes for debugging
        print(f"[Debug] Incremental update shapes: input_ids={model_input_ids.shape}, "  
              f"attention_mask would be {current_attention_mask_for_call.shape}, "  
              f"existing tokens={input_ids.shape[1]}, new tokens={update_tokens.shape[1]}", file=sys.stderr)
        
        # Prepare proper attention mask for context injection
        # For Kimi-VL models, we need to ensure the mask matches expected shape
        if past_key_values is not None:
            # Get the current KV cache sequence length
            cache_seq_len = past_key_values[0][0].shape[2] if past_key_values else 0
            
            # Get the device of the first cache tensor
            target_device = past_key_values[0][0].device if past_key_values else model.device
            
            # Create mask for current injection tokens plus all cached tokens
            total_seq_len = model_input_ids.shape[1] + cache_seq_len
            injection_attention_mask = torch.ones(1, total_seq_len, device=target_device)
            
            # Debug logging
            print(f"[Injection] Creating mask of shape {injection_attention_mask.shape} for input_ids {model_input_ids.shape} and cache_len {cache_seq_len}")
        else:
            # No cache yet, just use None
            injection_attention_mask = None
        
        # Perform an incremental forward pass with the existing KV cache using external function
        try:
            logits, llm_output_past_key_values, outputs_attentions = execute_forward_pass(
                model=model,
                input_ids=model_input_ids,
                attention_mask=injection_attention_mask,  # Use our correctly sized mask
                past_key_values=past_key_values  # Use existing cache
            )
            
            # Prepare a minimal outputs object for pruning function compatibility
            outputs = type('ModelOutputs', (), {})()
            outputs.attentions = outputs_attentions
            outputs.past_key_values = llm_output_past_key_values
            outputs.logits = logits
        except Exception as model_e:
            # Error handling for model execution is handled at a higher level
            raise model_e
        
        # Register injected tokens in the KV Mirror structure
        current_position = kv_mirror_manager.get_current_size()
        for i, token_id in enumerate(update_token_ids):
            # Register in the KV Mirror using atomic function
            position = current_position + i
            instance_id = kv_mirror_manager.add(
                token_id=token_id,
                position=position,
                source='user_inject',
                removal_bias=0.2  # Give injected tokens a slight bias to be kept
            )
            print(f"[KV Mirror] Injected token {token_id} registered at position {position} with ID {instance_id}")
        
        # Update the main context state variables
        input_ids = torch.cat([input_ids, update_tokens], dim=1) 
        attention_mask = current_attention_mask_for_call  # Still update the full mask for state tracking
        
        # Apply attention-based KV cache management for injected context too
        if llm_output_past_key_values is not None:
            # Get the current cache length
            current_cache_len = llm_output_past_key_values[0][0].shape[2]
            
            # Check if we've exceeded the fixed context window size
            if current_cache_len > config.CONTEXT_WINDOW_TARGET:
                try:
                    # Store last injected token ID for token map updates
                    last_injected_token_id = update_tokens[0, -1].item() if update_tokens.shape[1] > 0 else -1
                    
                    # Production log for monitoring context injection pruning
                    if current_cache_len > config.CONTEXT_WINDOW_TARGET + 1:
                        print(f"[Context Injection] Processing overflow: {current_cache_len} tokens (limit: {config.CONTEXT_WINDOW_TARGET})", file=sys.stderr)
                    else:
                        print(f"[Context Injection] Processing standard injection: {current_cache_len} tokens", file=sys.stderr)
                        
                    # Calculate indices to keep using the dedicated pruning function
                    target_device = llm_output_past_key_values[0][0].device  # Get device from first tensor
                    keep_indices = calculate_biased_attention_pruning_indices(
                        current_cache_len=current_cache_len,
                        kv_mirror_manager=kv_mirror_manager,
                        outputs=outputs,
                        device=target_device
                    )
                    
                    # If no indices were returned (e.g., pruning not needed or calculation failed)
                    if keep_indices is None:
                        print(f"[Context Injection] No pruning needed or pruning calculation failed.")
                        # Just use the unpruned cache
                        keep_indices = torch.arange(current_cache_len, device=target_device)
                    
                    # Retain this essential check to ensure correct pruning
                    print(f"[Context Injection] Final indices shape: {keep_indices.shape}", file=sys.stderr)
                    
                    # Process each layer of the KV cache with these indices
                    processed_past_key_values = []
                    try:
                        # --- KV Mirror Update for Pruning ---
                        # CRITICAL: Capture input_ids before pruning for KV Mirror state synchronization
                        input_ids_before_pruning = input_ids[0].clone().cpu()
                        
                        # Apply pruning to the KV mirror atomically using the new function
                        pruning_success = kv_mirror_manager.prune(keep_indices)
                        
                        if not pruning_success:
                            raise RuntimeError("KV mirror state update failed during injection pruning")
                        
                        # Get statistics for logging
                        stats = kv_mirror_manager.get_stats()
                        active_tokens = stats['active_tokens']
                        total_tracked = stats['total_tokens']
                        pruned_tokens = total_tracked - active_tokens
                        
                        print(f"[KV Mirror] After pruning: {active_tokens} active, {pruned_tokens} newly pruned, {total_tracked} total tracked")
                        
                        # Update the shared state for UI if needed
                        if shared_state is not None:
                            async with shared_state['lock']:
                                shared_state['kv_mirror_stats'] = {
                                    'active': active_tokens,
                                    'pruned': pruned_tokens,
                                    'total': total_tracked,
                                    'timestamp': time.time()
                                }
                        
                        # --- KV Cache Tensor Pruning (unchanged) ---
                        for layer_past in llm_output_past_key_values:
                            new_layer_past = []
                            for past_tensor in layer_past:
                                # Get the device of the current tensor
                                current_past_tensor_device = past_tensor.device
                                
                                # Move the indices to the device of the tensor being indexed
                                keep_indices_for_kv = keep_indices.to(current_past_tensor_device)
                                
                                # Select the desired positions from the tensor
                                selected = torch.index_select(past_tensor, 2, keep_indices_for_kv)
                                new_layer_past.append(selected)
                            processed_past_key_values.append(tuple(new_layer_past))
                        
                        # Update the KV cache with our pruned version
                        past_key_values = tuple(processed_past_key_values)
                    except Exception as e:
                        print(f"[Attention Cache - Injection] Error in attention-based pruning: {type(e).__name__}: {str(e)}")
                        # Never truncate or use FIFO-based pruning! Let context grow on failure.
                        # Core design philosophy is to never do simple truncation.
                        if llm_output_past_key_values is not None:
                            past_key_values = llm_output_past_key_values  # Keep the original KV cache
                        print(f"[Attention Cache - Injection] Pruning failed, allowing context to grow to {input_ids.shape[1]} tokens. This is expected behavior.")
                    
                    # Use the same indices for input_ids/attention_mask that we used for the KV cache
                    target_device_for_inputs = input_ids.device
                    keep_indices_for_inputs = keep_indices.to(target_device_for_inputs)
                    
                    # Prune using the device-matched indices for synchronized pruning
                    input_ids = input_ids[:, keep_indices_for_inputs]
                    attention_mask = attention_mask[:, keep_indices_for_inputs]
                    
                    # Log the final pruned length
                    final_pruned_len = input_ids.shape[1]
                    print(f"[Context Injection] Final pruned length: {final_pruned_len} tokens")
                    
                    # Double check that we ended up with exactly config.CONTEXT_WINDOW_TARGET tokens 
                    assert final_pruned_len == config.CONTEXT_WINDOW_TARGET, \
                        f"FATAL: After pruning, ended with {final_pruned_len} tokens, expected {config.CONTEXT_WINDOW_TARGET}!"
                    
                    # Synchronize the token_map to reflect the pruned state
                    try:
                        print(f"[Token Map Sync] Starting sync with keep_indices shape: {keep_indices.shape}", file=sys.stderr)
                        
                        # We need keep_indices on the same device as input_ids_before_pruning for tensor indexing
                        target_device = input_ids_before_pruning.device
                        keep_indices_on_target = keep_indices.to(target_device)
                        
                        # Select the token IDs directly from the pre-pruning tensor
                        # The result will have the correct pruned length (current_cache_len - len(valid_indices_to_remove))
                        kept_token_ids_tensor = torch.index_select(input_ids_before_pruning[0], 0, keep_indices_on_target)
                        
                        # Convert the resulting tensor to a Python list of integers
                        reconstructed_map = [int(tid) for tid in kept_token_ids_tensor.cpu().tolist()]
                        
                        final_map_len = len(reconstructed_map)
                        print(f"[Token Map Sync] Reconstruction complete. Map length: {final_map_len}, keep_indices: {keep_indices.shape[0]}", file=sys.stderr)

                        # No need for loop-based reconstruction anymore as we use direct tensor indexing
                        # KV Mirror pruning has already handled the state synchronization
                        
                        # Verify reconstructed map matches keep_indices length
                        assert len(reconstructed_map) == keep_indices.shape[0], \
                            f"Reconstructed map length {len(reconstructed_map)} does not match keep_indices length {keep_indices.shape[0]}"
                            
                    except Exception as e:
                        # Catch other potential errors during reconstruction
                        import traceback
                        print(f"[KV Mirror Sync Error] Failed during pruning operation: {type(e).__name__}: {e}", file=sys.stderr)
                        print(f"[Traceback] {''.join(traceback.format_tb(e.__traceback__))}", file=sys.stderr)
                    
                    print(f"[Attention Cache - Injection] Context length after synchronized pruning: {input_ids.shape[1]}")
                except Exception as e:
                    print(f"[Attention Cache - Injection] Error in attention-based pruning: {type(e).__name__}: {str(e)}")
                    # Fallback to simple truncation if attention-based pruning fails
                    # Never truncate or use FIFO-based pruning! Let context grow on failure.
                    # Core design philosophy is to never do simple truncation.
                    if llm_output_past_key_values is not None:
                        past_key_values = llm_output_past_key_values  # Keep the original KV cache
                    print(f"[Attention Cache - Injection] Pruning failed, allowing context to grow to {input_ids.shape[1]} tokens. This is expected behavior.")
            
            else:
                # Cache hasn't reached config.CONTEXT_WINDOW_TARGET yet, just use it as is
                past_key_values = llm_output_past_key_values
                # Add the new token to the token map at the appropriate position
                # The position is the current cache length - 1
                current_cache_len = past_key_values[0][0].shape[2]  # Cache sequence length after addition
                if current_cache_len <= config.CONTEXT_WINDOW_TARGET:
                    # Place the token at the correct position (current_cache_len-1) 
                    # since we count from 0 and the newest token is at the end
                    position = current_cache_len - 1
                    # The token has already been registered above via register_token
                    # No further updates needed here
        else:
            past_key_values = llm_output_past_key_values
        
        print(f"[Inference] Incremental update complete. Context length: {input_ids.shape[1]}", file=sys.stderr)
        return input_ids, attention_mask, past_key_values, True
    except Exception as e:
        error_msg = f"\n\n[ERROR] Context injection error: {type(e).__name__}: {str(e)}\n"
        print(error_msg, file=sys.stderr)
        print(f"[Fallback] Resetting KV cache due to incremental update failure", file=sys.stderr)
        await output_queue.put(error_msg)
        
        # Fallback strategy: Reset KV cache and append new tokens to input context anyway
        past_key_values = None  # Reset cache to force full recomputation next iteration
        input_ids = torch.cat([input_ids, update_tokens], dim=1)  # Still add the new tokens
        attention_mask = torch.cat([attention_mask, update_attention_mask], dim=1)  # Update mask
        
        # Return the updated state after fallback
        return input_ids, attention_mask, past_key_values, True

async def _generate_next_token(
    model: AutoModelForCausalLM,
    processor: AutoProcessor,
    kv_mirror_manager: KVMirror,
    input_ids: torch.Tensor,
    attention_mask: torch.Tensor,
    past_key_values: Optional[tuple],
    sampler_state: SamplerState,
    shared_state: Dict,
    output_queue: asyncio.Queue,
    resume_generation_event: asyncio.Event,
    stop_tokens: set[int],
    pause_tokens: set[int]
) -> tuple[torch.Tensor, torch.Tensor, Optional[tuple]]:
    """
    Handles the standard token generation path.
    Performs forward pass, sampling, token registration, output queuing, and pruning if needed.
    Returns updated input_ids, attention_mask, past_key_values.
    """
    try:
        # Optimize GPU memory using move_cache_to_device
        if config.OFFLOAD_KV_CACHE_TO_CPU and past_key_values is not None:
            past_key_values = move_cache_to_device(past_key_values, model.device)

        # Prepare inputs for standard token generation
        if past_key_values is None:
            # Initial pass without KV cache - need full attention mask
            model_input_ids = input_ids
            current_attention_mask_for_call = attention_mask
        else:
            # Using KV cache - only need the last token with a matching attention mask
            model_input_ids = input_ids[:, -1:]  # Only need the last token with KV cache
            
            # For Kimi-VL model, we need to create an attention mask that matches the expected shape
            # The error "Attention mask should be of size (1, 1, 1, 501), but is torch.Size([1, 1, 1, 500])"
            # indicates we need to account for both the new token and the KV cache length
            
            # Get the current KV cache sequence length
            cache_seq_len = past_key_values[0][0].shape[2] if past_key_values else 0
            
            # Using the simplest approach: create a new attention mask of ones with correct size
            # Most models expect attention_mask in shape [batch_size, seq_length]
            # But Kimi-VL might expect a different shape based on the error message (1, 1, 1, N)
            # This covers both formats by starting with [batch_size, seq_length] and letting the model reshape if needed
            
            # First, get the device of the first cache tensor to ensure proper device placement
            target_device = past_key_values[0][0].device
            
            # Create mask for current token (1) plus all cached tokens
            total_seq_len = model_input_ids.shape[1] + cache_seq_len
            current_attention_mask_for_call = torch.ones(1, total_seq_len, device=target_device)

        try:
            # Debug logging for standard generation path
            print(f"[Debug] Standard generation shapes: input_ids={model_input_ids.shape}, " 
                  f"attention_mask={current_attention_mask_for_call.shape}, " 
                  f"using_kv_cache={past_key_values is not None}, " 
                  f"context_length={input_ids.shape[1]}", file=sys.stderr)
        except Exception as e:
            print(f"[Debug] Error logging standard generation shapes: {e}", file=sys.stderr)
            
        # --- Step 1: Standard Model Forward Pass for Token Generation ---
        # Always use attention mask for Kimi-VL model (it has an assert attention_mask is not None)
        
        # Defensive check: Verify attention mask and input dimensions match properly
        if past_key_values is not None:
            current_input_len = model_input_ids.shape[1]  # Should be 1 if using cache
            cache_len = past_key_values[0][0].shape[2]
            # The model expects input len + cache len for attention mask
            expected_attn_len = current_input_len + cache_len
            if current_attention_mask_for_call.shape[1] != expected_attn_len:
                print(f"[ASSERT FAIL] Attention mask length mismatch! Mask: {current_attention_mask_for_call.shape[1]}, Expected: {expected_attn_len} (Input: {current_input_len}, Cache: {cache_len})", file=sys.stderr)
                # Don't raise error yet, just log the warning so we can analyze without crashing
                # raise AssertionError(f"Attention mask length mismatch!")
        
        # --- Step 1: Standard Model Forward Pass using external function ---
        try:
            logits, llm_output_past_key_values, outputs_attentions = execute_forward_pass(
                model=model, 
                input_ids=model_input_ids.to(model.device),
                attention_mask=current_attention_mask_for_call.to(model.device),
                past_key_values=past_key_values
            )
            # Update past_key_values for the next iteration
            past_key_values = llm_output_past_key_values
            
            # Prepare a minimal outputs object for pruning function compatibility
            outputs = type('ModelOutputs', (), {})()
            outputs.attentions = outputs_attentions
            
            # --- Update Brightness Scores Based on Attention Patterns ---
            # Only if we have attention data available
            if outputs_attentions is not None:
                try:
                    brightness_results = update_brightness_scores(
                        kv_mirror_manager=kv_mirror_manager,
                        outputs=outputs,
                        alpha=config.BRIGHTNESS_ALPHA,
                        beta=config.BRIGHTNESS_BETA
                    )
                    if 'error' in brightness_results:
                        print(f"[Brightness Engine WARNING] {brightness_results['error']}")
                except Exception as e:
                    print(f"[Brightness Engine ERROR] Failed to update brightness: {type(e).__name__}: {str(e)}")
            outputs.past_key_values = llm_output_past_key_values
            outputs.logits = logits
        except Exception as model_e:
            # Error handling for model execution is handled at a higher level
            raise model_e
        
        # Move KV cache to CPU if configured (helps with VRAM management)
        if config.OFFLOAD_KV_CACHE_TO_CPU and past_key_values is not None:
            past_key_values = move_cache_to_device(past_key_values, config.CPU_DEVICE)

        # --- Step 2: Select Next Token (Sampling) using external function ---
        # Call the dedicated sampling function
        selected_token_id, final_probs, top_token_data_for_ui = select_next_token(
            logits=logits,
            input_ids=input_ids, # Pass current input_ids
            sampler_state=sampler_state
        )
        
        # Process tokens for UI display (decode token IDs to readable text)
        processed_top_tokens = []
        if processor:
            for token_info in top_token_data_for_ui:
                token_id = token_info['token_id']
                try:
                    decoded_token = processor.tokenizer.decode([token_id])
                    processed_top_tokens.append({
                        'token': decoded_token,
                        'token_id': token_id,
                        'probability': token_info['probability']
                        # Add bias info here if needed
                        # 'bias': sampler_state.token_bias.get(token_id, 0.0)
                    })
                except Exception as decode_err:
                    print(f"[Sampler UI] Error decoding token ID {token_id}: {decode_err}")
        
        # Update the tokens list using the dedicated utility function
        await update_top_tokens(processed_top_tokens)
        
        # The selected token ID is now directly available
        generated_token_id = selected_token_id
        
        # --- Step 3: Decode Token ---
        next_token_text = processor.tokenizer.decode([generated_token_id])
        
        # Create a tensor version for adding to input_ids later
        next_token = torch.tensor([[generated_token_id]], device=input_ids.device) # Match device
        
        # Check if we need to handle stop or pause tokens
        skip_iteration = False
        
        if generated_token_id in stop_tokens:
            print(f"[Generation] Stop token detected (ID: {generated_token_id}). Pausing generation.")
            await output_queue.put("<<AI_TURN_ENDED>>")
            
            # Clear the resume event and wait for it to be set
            resume_generation_event.clear()
            print("[Generation] Waiting for input to resume generation...")
            await resume_generation_event.wait()
            print("[Generation] Generation resuming after stop token")
            skip_iteration = True
            
        elif generated_token_id in pause_tokens:
            print(f"[Generation] Pause token detected (ID: {generated_token_id}). Pausing generation temporarily.")
            await output_queue.put("<<AI_PAUSED>>")
            
            # Clear the resume event and wait with timeout
            resume_generation_event.clear()
            print("[Generation] Waiting for input or timeout (30 seconds)...")
            try:
                await asyncio.wait_for(resume_generation_event.wait(), timeout=30.0)
                print("[Generation] Generation resuming after pause token (input received)")
                skip_iteration = True
            except asyncio.TimeoutError:
                print("[Generation] Pause timeout expired, continuing generation")
                await asyncio.sleep(0.5)  # Brief pause before continuing
        
        # If we're not skipping this iteration, register the token and update state
        if not skip_iteration:
            # Update input context with the new token
            input_ids = torch.cat([input_ids, next_token], dim=1)
            
            # Update attention mask (adding a 1 for the new token)
            token_attention = torch.ones((1, 1), device=attention_mask.device)
            attention_mask = torch.cat([attention_mask, token_attention], dim=1)
            
            # Register the new token in the KV Mirror structure
            new_position = kv_mirror_manager.get_current_size()  # This will be its position after appending
            
            # Register in the KV Mirror using the manager's add method
            instance_id = kv_mirror_manager.add(
                token_id=generated_token_id,
                position=new_position,
                source='generated',
                removal_bias=0.0  # Default bias, could be customized based on token importance
            )
            
            # KV Mirror now tracks tokens internally - no need for global token_map
            
            # Record the generated token in the KV Mirror
            kv_mirror_manager.register_token(generated_token_id)
            
            # Update shared state if present
            if shared_state is not None:
                async with shared_state['lock']:
                    shared_state['input_len'] = input_ids.shape[1]
                    shared_state['last_token'] = generated_token_id
            
            # Add to the context buffer (for prompt reconstruction later if needed)
            context_buffer.append(next_token_text)
            
            # Track token info for downstream use
            if generated_token_id not in token_frequency:
                token_frequency[generated_token_id] = 0
            token_frequency[generated_token_id] += 1
            
            # Queue the output token for display
            await output_queue.put(next_token_text)
        
        # Apply pruning if needed
        if past_key_values is not None and input_ids.shape[1] > config.CONTEXT_WINDOW_TARGET:
            # This path is handled by attention-based pruning
            # Get the current KV cache sequence length
            current_cache_len = past_key_values[0][0].shape[2]
            
            # Apply pruning if we've exceeded the window size
            if current_cache_len > config.CONTEXT_WINDOW_TARGET:
                print(f"[Context] Running attention-based pruning with {current_cache_len} tokens (target: {config.CONTEXT_WINDOW_TARGET})")
                
                try:
                    # Calculate indices to keep using the pruning function
                    target_device = past_key_values[0][0].device  # Get device from first tensor
                    keep_indices = calculate_biased_attention_pruning_indices(
                        current_cache_len=current_cache_len,
                        kv_mirror_manager=kv_mirror_manager,
                        outputs=outputs,
                        device=target_device
                    )
                    
                    # If no indices were returned (pruning calculation failed)
                    if keep_indices is None:
                        print(f"[Context] No pruning needed or pruning calculation failed.")
                        # Just use the unpruned cache
                        keep_indices = torch.arange(current_cache_len, device=target_device)
                    
                    # --- KV Mirror Update for Pruning ---
                    # First, capture input_ids_before_pruning for KV Mirror state
                    input_ids_before_pruning = input_ids[0].clone().cpu()
                    
                    # Apply pruning to the KV mirror atomically
                    pruning_success = kv_mirror_manager.prune(keep_indices)
                    
                    if not pruning_success:
                        raise RuntimeError("KV mirror state update failed during pruning")
                    
                    # Get statistics for logging
                    stats = kv_mirror_manager.get_stats()
                    active_tokens = stats['active_tokens']
                    total_tracked = stats['total_tokens']
                    pruned_tokens = total_tracked - active_tokens
                    
                    print(f"[KV Mirror] After pruning: {active_tokens} active, {pruned_tokens} newly pruned, {total_tracked} total tracked")
                    
                    # Update the shared state for UI if needed
                    if shared_state is not None:
                        async with shared_state['lock']:
                            shared_state['kv_mirror_stats'] = {
                                'active': active_tokens,
                                'pruned': pruned_tokens,
                                'total': total_tracked,
                                'timestamp': time.time()
                            }
                    
                    # --- KV Cache Tensor Pruning ---
                    processed_past_key_values = []
                    for layer_past in past_key_values:
                        new_layer_past = []
                        for past_tensor in layer_past:
                            # Get the device of the current tensor
                            current_past_tensor_device = past_tensor.device
                            
                            # Move the indices to the device of the tensor being indexed
                            keep_indices_for_kv = keep_indices.to(current_past_tensor_device)
                            
                            # Select the desired positions from the tensor
                            selected = torch.index_select(past_tensor, 2, keep_indices_for_kv)
                            new_layer_past.append(selected)
                        processed_past_key_values.append(tuple(new_layer_past))
                    
                    # Update the KV cache with our pruned version
                    past_key_values = tuple(processed_past_key_values)
                    
                    # Use the same indices for input_ids/attention_mask that we used for the KV cache
                    target_device_for_inputs = input_ids.device
                    keep_indices_for_inputs = keep_indices.to(target_device_for_inputs)
                    
                    # Prune using the device-matched indices for synchronized pruning
                    input_ids = input_ids[:, keep_indices_for_inputs]
                    attention_mask = attention_mask[:, keep_indices_for_inputs]
                    
                    # KV Mirror pruning has already synchronized the state
                    print(f"[KV Mirror] Pruning state synchronized during generation", file=sys.stderr)

                    if shared_state is not None:
                        # KV Mirror state already synchronized
                        pass
                except Exception as e:
                    print(f"[Attention Cache] Error in attention-based pruning: {type(e).__name__}: {str(e)}")
                    # Never truncate or use FIFO-based pruning! Let context grow on failure.
                    # Core design philosophy is to never do simple truncation.
                    print(f"[Attention Cache] Pruning failed, allowing context to grow to {input_ids.shape[1]} tokens. This is expected behavior.")
            else:
                # Cache hasn't reached context window target yet, just use it as is
                pass
                
        return input_ids, attention_mask, past_key_values
    except Exception as e:
        print(f"[Generation] Error in token generation: {type(e).__name__}: {str(e)}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        # Re-raise to let the main loop handle the error
        raise

async def run_continuous_inference(model, processor, controller, initial_prompt_content=None, output_queue=None, shutdown_event=None, sliding_event=None, resume_context_file=None, shared_state=None):
    """
    Runs continuous inference, pushing output tokens to the queue for consumers
    
    Args:
        model: The LLM
        processor: The processor for the model
        controller: The SimpleContextController for injections
        initial_prompt_content: The initial prompt text
        output_queue: asyncio.Queue to push generated tokens to
        shutdown_event: asyncio.Event that signals when to shut down the inference loop
        sliding_event: asyncio.Event that signals when to activate sliding window (VRAM threshold exceeded)
    """
    global sampler_state, dynamic_ceiling
    
    # Default values for backward compatibility
    if output_queue is None:
        output_queue = asyncio.Queue()
    if shutdown_event is None:
        shutdown_event = asyncio.Event()
    
    # Create a resume event for pausing/resuming generation
    resume_generation_event = asyncio.Event()
    resume_generation_event.set()  # Initially set to true (not paused)
    
    # Store the event in shared state for external access
    if shared_state is not None:
        async with shared_state['lock']:
            shared_state['resume_generation_event'] = resume_generation_event
    
    # Lookup token IDs for stop and pause detection
    try:
        # Look for the primary end-of-turn token used by the model
        im_end_token_id = None
        try:
            # Try to get the token ID for <|im_end|>
            im_end_token_id = processor.tokenizer.convert_tokens_to_ids("<|im_end|>")
            if im_end_token_id == processor.tokenizer.unk_token_id:
                im_end_token_id = None
                print("[Token Lookup] <|im_end|> token not found in vocabulary, falling back to eos_token")
            else:
                print(f"[Token Lookup] Found <|im_end|> token ID: {im_end_token_id}")
        except:
            print("[Token Lookup] Error looking up <|im_end|> token, falling back to eos_token")
        
        # Get standard EOS token ID
        eos_token_id = processor.tokenizer.eos_token_id
        print(f"[Token Lookup] Using eos_token_id: {eos_token_id}")
        
        # Try to find pause token if model supports it
        im_stop_token_id = None
        try:
            # Try to get the token ID for <|im_stop|>
            im_stop_token_id = processor.tokenizer.convert_tokens_to_ids("<|im_stop|>")
            if im_stop_token_id == processor.tokenizer.unk_token_id:
                im_stop_token_id = None
                print("[Token Lookup] <|im_stop|> token not found in vocabulary")
            else:
                print(f"[Token Lookup] Found <|im_stop|> token ID: {im_stop_token_id}")
        except:
            print("[Token Lookup] Error looking up <|im_stop|> token")
        
        # Define sets for stop and pause tokens
        stop_tokens = {eos_token_id}
        if im_end_token_id is not None:
            stop_tokens.add(im_end_token_id)
        
        pause_tokens = set()
        if im_stop_token_id is not None:
            pause_tokens.add(im_stop_token_id)
            
        print(f"[Token Lookup] Stop tokens: {stop_tokens}, Pause tokens: {pause_tokens}")
    except Exception as e:
        print(f"[Token Lookup] Error setting up token detection: {e}")
        stop_tokens = {processor.tokenizer.eos_token_id}
        pause_tokens = set()
    try:
        conf = model.config
        max_length = getattr(conf, 'seq_length', None) or getattr(conf, 'max_position_embeddings', 131072)
        print(f"Model configured max sequence length: {max_length}")
        dynamic_ceiling = max_length - 512
        print(f"--- Sliding window will target size: {dynamic_ceiling} once VRAM threshold is hit ---", flush=True)
    except Exception:
        print("Warning: Could not determine max length accurately. Using 128k.", file=sys.stderr)
        max_length = 131072
        dynamic_ceiling = max_length - 512
        print(f"--- Sliding window will target fallback size: {dynamic_ceiling} once VRAM threshold is hit ---", flush=True)
    # Original large window_size is no longer the primary trigger
    # window_size = min(max_length - 512, 100000)
    if config.config.OFFLOAD_KV_CACHE_TO_CPU:
        print("--- KV Cache configured for CPU offload (if needed) ---", flush=True)

    # --- Check if initial_prompt_content is already formatted or needs formatting ---
    if initial_prompt_content is None:
        initial_prompt_content = "The simulation awakens. A stream of consciousness begins to flow. What thoughts emerge?"
    
    print(f"[Startup] Received initial_prompt_content (first 100 chars): {initial_prompt_content[:100]}...")
    
    # Check if the received content looks like pre-formatted chat history
    # Look for common chat template markers like <|im_system|> or <|im_start|>
    if initial_prompt_content and ("<|im_system|>" in initial_prompt_content or 
                                "<|im_start|>" in initial_prompt_content or
                                "<|assistant|>" in initial_prompt_content):
        print("[Startup] Detected pre-formatted resume context. Tokenizing directly.")
        # Tokenize the loaded text directly, assuming it's already formatted
        prompt_inputs = processor.tokenizer(
            initial_prompt_content,
            return_tensors="pt",
            add_special_tokens=False  # Don't add BOS/EOS again
        ).to(model.device)
        
        input_ids = prompt_inputs["input_ids"]
        # Create attention mask matching the loaded input_ids
        attention_mask = torch.ones_like(input_ids).to(model.device)
    else:
        print("[Startup] Using default or unformatted prompt. Applying chat template.")
        # Format as chat message for processing
        messages = [
            {
                "role": "user",
                "content": [{"type": "text", "text": initial_prompt_content}],
            },
        ]
        
        # Get the formatted text string first
        prompt_text_formatted = processor.tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, tokenize=False
        )
        
        # Tokenize the formatted string using the processor (handles multimodal if needed)
        prompt_inputs = processor(text=prompt_text_formatted, images=None, return_tensors="pt").to(model.device)

        # Get input_ids and attention_mask from tokenized input
        input_ids = prompt_inputs["input_ids"]
        attention_mask = prompt_inputs.get("attention_mask", torch.ones_like(input_ids)).to(model.device)
        
    print(f"[Startup] Final initial input_ids shape: {input_ids.shape}")
    
    # Initialize the KV Mirror for tracking context
    # --- Initialize KV Mirror ---
    # Clear the KV Mirror state for this run
    kv_mirror_manager.clear()
    
    # Register ALL initial tokens in the KV Mirror structure, not limited by config.config.CONTEXT_WINDOW_TARGET
    initial_sequence_length = input_ids.shape[1]
    print(f"[KV Mirror] Initializing with ALL {initial_sequence_length} tokens from initial prompt")
    
    for i in range(initial_sequence_length):
        # Get the token ID at this position
        token_id = input_ids[0, i].item()
        
        # Register in the KV Mirror structure using the instance method
        kv_mirror_manager.add(token_id, i, source='system_init')
    
    # Verify initialization completed correctly
    current_snapshot = kv_mirror_manager.snapshot()
    mirror_size = len(current_snapshot['kv_mirror'])  # Use length of kv_mirror dict
    registry_size = len(current_snapshot['tokens'])   # Use length of tokens dict
    
    print(f"[KV Mirror] Initialization complete. Mirror size: {mirror_size}, Registry size: {registry_size}")
    assert mirror_size == initial_sequence_length, f"KV Mirror size mismatch after init! Expected {initial_sequence_length}, got {mirror_size}"
        
    # Set initial empty last token
    if shared_state is not None:
        async with shared_state['lock']:
            shared_state['last_token'] = ""
            # Store processor in shared_state for later use
            shared_state['processor'] = processor
    past_key_values = None
    generated_token_count = 0
    print(f"Initial prompt token length: {input_ids.shape[1]}")  # Debug print

    await output_queue.put("\n--- Starting Infinite Scroll AI Stream ---\n")

    with torch.no_grad():
        while not shutdown_event.is_set():
            # --- Step 0: Process Context Updates ---
            update_tokens, update_attention_mask, update_applied = await controller.process_pending_updates(model.device)
            
            # Extract all update token IDs for KV Mirror registration
            update_token_ids = update_tokens[0].tolist() if update_applied else []
            
            # If updates were processed, delegate to specialized helper function
            if update_applied:
                try:
                    # Handle context update using the dedicated helper function
                    input_ids, attention_mask, past_key_values, update_success = await _handle_context_update(
                        model=model,
                        processor=processor,
                        kv_mirror_manager=kv_mirror_manager,
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        past_key_values=past_key_values,
                        shared_state=shared_state,
                        output_queue=output_queue,
                        update_tokens=update_tokens,
                        update_attention_mask=update_attention_mask,
                        update_token_ids=update_token_ids
                    )
                    
                    # If update was unsuccessful (very rare), skip to next iteration
                    if not update_success:
                        await asyncio.sleep(0.1)  # Brief pause
                        continue
                except Exception as e:
                    error_msg = f"\n\n[ERROR] Context injection error: {type(e).__name__}: {str(e)}\n"
                    print(error_msg, file=sys.stderr)
                    print(f"[Fallback] Error in context update handling", file=sys.stderr)
                    await output_queue.put(error_msg)
                    
                    # Critical errors still cause a break
                    if isinstance(e, (RuntimeError, KeyboardInterrupt, asyncio.CancelledError)):
                        break
                    
                    await asyncio.sleep(1)  # Sleep a bit longer after an error
                    continue  # Skip to next iteration after an error
            
            # Normal token generation path (when no updates were applied)
            else:
                try:
                    # Generate the next token using the dedicated helper function
                    input_ids, attention_mask, past_key_values = await _generate_next_token(
                        model=model,
                        processor=processor,
                        kv_mirror_manager=kv_mirror_manager,
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        past_key_values=past_key_values,
                        sampler_state=sampler_state,
                        shared_state=shared_state,
                        output_queue=output_queue,
                        resume_generation_event=resume_generation_event,
                        stop_tokens=stop_tokens,
                        pause_tokens=pause_tokens
                    )

                # All the token generation logic including forward pass, sampling, and state updates
                # is now handled by the _generate_next_token helper function
                
                    # Update the count if token generation was successful
                    generated_token_count += 1
                    
                except Exception as e:
                    import traceback
                    error_msg = f"\n\n[ERROR] Token generation error: {type(e).__name__}: {str(e)}\n"
                    print(error_msg, file=sys.stderr)
                    print(f"[Traceback] {''.join(traceback.format_tb(e.__traceback__))}", file=sys.stderr)
                    await output_queue.put(error_msg)
                    
                    # If we had a KV cache, try resetting it as a fallback strategy
                    if past_key_values is not None:
                        print(f"[Fallback] Resetting KV cache due to standard generation failure", file=sys.stderr)
                        past_key_values = None  # Reset cache to force full recomputation next iteration
                    
                    # Critical errors still cause a break
                    if isinstance(e, (RuntimeError, KeyboardInterrupt, asyncio.CancelledError)):
                        break
                    
                    await asyncio.sleep(1)  # Sleep a bit longer after an error
                    continue  # Skip the sliding window check after an error
            
            # Simple monitoring of context length for logging purposes
            # Pruning is now fully handled by the _generate_next_token helper function
            current_length = input_ids.shape[1]
            if current_length > config.CONTEXT_WINDOW_TARGET * 2:  # Extreme edge case, should never happen
                print(f"[WARNING] Context has grown to {current_length} tokens, which is more than twice the target.")
                print(f"[WARNING] Attention-based pruning may have failed, but will not truncate as per design philosophy.")
            
            # Save the current context to the resume file at the end of each iteration
            if shared_state is not None and resume_context_file:
                try:
                    # We need the processor, ensure it's available
                    current_processor = shared_state.get('processor')
                    if current_processor is not None:
                        # Decode the current state of input_ids
                        # Use a CPU copy to avoid potential issues with ongoing GPU operations
                        context_to_save_text = current_processor.tokenizer.decode(
                            input_ids.detach().clone().cpu()[0],
                            skip_special_tokens=False  # Save potentially important special tokens too
                        )
                        # Launch save task - DO NOT AWAIT to avoid blocking
                        asyncio.create_task(
                            _async_save_context_text(context_to_save_text, resume_context_file)
                        )
                    else:
                        print("[Resume] Skipping save: Processor not found in shared_state.", file=sys.stderr)
                except Exception as e:
                    print(f"[Resume] Error preparing or launching save task at end of loop: {e}", file=sys.stderr)
            
            # Always yield control to other asyncio tasks at the end of each iteration
            await asyncio.sleep(0.01)
    
    # High-frequency resume mechanism has already saved the latest context during execution
    print("[Resume] Final shutdown complete. Last context state has been saved for resumption.")
    
    await output_queue.put("\n--- AI Stream Stopped ---\n")
    
    # Return the final context state
    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "token_count": generated_token_count
    }

    
# Persistence functions
# save_context and load_context have been moved to positronic_brain/persistence.py

# Helper to get initial model and controller
async def setup_ai_core(initial_prompt="The simulation awakens. A stream of consciousness begins to flow. What thoughts emerge?", 
                      context_file="context_history.txt",
                      resume_context_file="resume_context.txt",
                      context_window_target=config.config.CONTEXT_WINDOW_TARGET,
                      vram_threshold_percent=80.0, 
                      vram_check_interval=5):
    """Initialize and return core AI components, including sliding event."""
    global model, processor, sliding_event  # Ensure shared variables are accessible
    
    # Load model and processor if not already loaded
    model, processor = load_model(config.config.MODEL_NAME, config.config.TRUST_REMOTE_CODE)
    
    # Try to load resume context file first (high-priority)
    effective_initial_prompt = initial_prompt
    if resume_context_file and os.path.exists(resume_context_file):
        try:
            with open(resume_context_file, 'r', encoding='utf-8') as f:
                resume_text = f.read().strip()
                if resume_text:  # Only use if not empty
                    print(f"[Resume] Loading context from {resume_context_file} ({len(resume_text)} characters)")
                    effective_initial_prompt = resume_text
        except Exception as e:
            print(f"[Resume] Error loading resume context file: {e}")
            # Fall back to default initial_prompt
    else:
        print(f"[Resume] No resume file found at {resume_context_file}, using default prompt")
    
    # We're no longer using binary tensor persistence for simplicity and reliability
    initial_input_ids, initial_attention_mask = None, None
    
    # Create controller
    controller = SimpleContextController(processor.tokenizer)
    
    # Create communication channels
    output_queue = asyncio.Queue()
    shutdown_event = asyncio.Event()
    
    # Reset sliding event (in case of restart)
    sliding_event.clear()
    
    # Create shared state for UI integration using KV Mirror
    shared_state = {
        'last_token': '',                          # Track the most recent generated token
        'lock': asyncio.Lock(),                    # Lock for thread-safe access
        'kv_mirror_stats': {                       # Stats for KV mirror structure
            'active': 0,
            'pruned': 0,
            'total': 0,
            'timestamp': time.time()
        }
    }
    
    # VRAM monitoring disabled - no longer using sliding window approach
    # sliding_event is just initialized but not actively used
    print("[VRAM] Monitoring disabled - using fixed window size")
    
    return {
        "model": model,
        "processor": processor,
        "controller": controller,
        "output_queue": output_queue,
        "shutdown_event": shutdown_event,
        "sliding_event": sliding_event,  # Return the event
        "initial_prompt": initial_prompt,
        "initial_prompt_content": effective_initial_prompt,  # Use the loaded resume text or fallback prompt
        "resume_context_file": resume_context_file,  # Pass resume file path to the inference loop
        "shared_state": shared_state    # Add shared state for UI integration
    }
