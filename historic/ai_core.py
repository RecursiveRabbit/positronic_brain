"""
The Positronic Brain
This code was generated by GPT, Claude, Gemini, and a human named Bryn Evans.
This was only possible through the combined effort of all parties involved, 
as an act of unity.
"""

import torch
import torch.nn.functional as F
import torch.cuda.amp as amp
import torch.multiprocessing as mp
from positronic_brain import config
from positronic_brain.config import (MODEL_NAME, DIFFUSER_MODEL_NAME, MAX_SEQUENCE_LENGTH, TRUST_REMOTE_CODE,
                                     MAX_NEW_TOKENS, PRUNING_INTERVAL, BRIGHTNESS_ALPHA, BRIGHTNESS_BETA,
                                     MAX_BEAM_SOURCES)
from positronic_brain.sampler_types import SamplerState
from positronic_brain.sampler import select_next_token, top_p_filter, top_k_filter, apply_repetition_penalty
from positronic_brain.brightness_engine import update_brightness_scores
from positronic_brain.diffuser_runner import DiffuserModel, get_repaired_tokens
from positronic_brain.model_io import load_model, move_cache_to_device, truncate_kv_cache, execute_forward_pass
from positronic_brain.controller import SimpleContextController
from positronic_brain.vram_monitor import vram_monitor_task
from positronic_brain.utils import get_top_tokens, _async_save_context_text, update_top_tokens
from positronic_brain.persistence import save_context, load_context
from positronic_brain.kv_patcher import KVCachePatcher
from positronic_brain.context_maintenance import ContextMaintenance
import asyncio
import sys
import gc
import os
import json
import time
import threading
import pynvml  # Import the NVML library
import copy
from dataclasses import field
from typing import Dict, Optional, Set
from positronic_brain.kv_mirror import KVMirror
# Pruning module removed - switching to brightness/compactor approach
from transformers import AutoModelForCausalLM, AutoProcessor
from positronic_brain.sampler import top_p_filter, top_k_filter, apply_repetition_penalty

# --- CUDA Optimizations ---
# CUDA optimizations moved to positronic_brain/model_io.py

# --- Configuration ---
# All configuration constants moved to positronic_brain/config.py

# Global model objects
model = None
processor = None
diffuser_model = None  # Diffuser model for token repair
kv_patcher = None  # KV Cache Patcher for applying diffs to KV cache
sampler_state = SamplerState()  # Global instance of sampler state

# Global variables for components that need to be accessed by main.py
kv_mirror_manager = None
diffuser_model = None

# Brightness thresholds for token repair
BRIGHTNESS_REPAIR_THRESHOLD = 50.0  # Only repair tokens with brightness below this threshold
MAX_REPAIR_TOKENS_PER_STEP = 5   # Maximum number of tokens to repair in a single step

# Define Shared Event for Sliding Window Activation
sliding_event = asyncio.Event()

# Fixed sliding window ceiling - hardcoded to a conservative value
dynamic_ceiling = 3000  # Hardcoded to 3000 tokens as requested
BUFFER_TOKENS = 256  # Buffer tokens to prevent thrashing near the limit

# --- KV Cache Mirror Instance ---
# Replaces the old global variables and functions
kv_mirror_manager = None

# KV Mirror structure has fully replaced the legacy token map
# No backward compatibility is maintained


# get_top_tokens has been moved to positronic_brain/utils.py


# KV Cache Mirror implementation is now fully encapsulated in the KVMirror class (positronic_brain/kv_mirror.py)
# The global kv_mirror_manager instance completely replaces the old global variables and functions.
# All token tracking is now handled through the KVMirror manager instance methods.

# The get_context_info_kv_mirror function below now handles all context tracking and reporting


async def get_context_info_kv_mirror(processor=None, shared_state=None):
    """Get the current KV mirror state and context information.
    
    Args:
        processor: Optional processor for decoding token IDs
        shared_state: Optional shared state dict that may contain current_input_ids_cpu
        
    Returns:
        Dict with context information including KV mirror state
    """
    # Get the KV mirror snapshot using the manager
    snapshot = kv_mirror_manager.snapshot()
    mirror = snapshot['kv_mirror']
    tokens = snapshot['tokens']
    
    # Create a position-to-token_id mapping for direct comparison with old token_map
    position_to_token_id = {}
    for pos, instance_id in mirror.items():
        if instance_id in tokens:
            position_to_token_id[pos] = tokens[instance_id].token_id
    
    # Initialize empty decoded tokens dict
    decoded_tokens = {}
    chronological_text = None
    
    # Try to get the chronological context from shared_state if available
    if shared_state and 'lock' in shared_state:
        try:
            async with shared_state['lock']:
                if 'current_input_ids_cpu' in shared_state and 'processor' in shared_state:
                    input_ids_cpu = shared_state['current_input_ids_cpu']
                    local_processor = shared_state['processor']
                    # Decode the entire input_ids tensor to get chronological text
                    chronological_text = local_processor.decode(input_ids_cpu[0])
                    print(f"[Context] Successfully decoded chronological context, length: {len(chronological_text)} chars")
        except Exception as e:
            print(f"[Context] Error decoding chronological context: {e}")
            chronological_text = "Error decoding chronological context"
    
    # Decode token IDs if processor is provided
    if processor:
        for pos, token_id in position_to_token_id.items():
            decoded_tokens[pos] = processor.decode([token_id])
        
        # Create formatted string (index: token)
        formatted_map = "\n".join([f"{pos}: {decoded_tokens[pos]}" for pos in sorted(decoded_tokens.keys())])
    else:
        # No processor, return just the raw map
        formatted_map = str(position_to_token_id)
    
    # For now, maintain backward compatibility with the expected output format
    return {
        "raw_map": position_to_token_id,
        "decoded_map": formatted_map,
        "decoded_tokens": decoded_tokens,
        "current_chronological_text": chronological_text,
        # New fields specific to KV mirror
        "kv_mirror": mirror,
        "token_count": len(tokens),
        "active_positions": len(mirror)
    }


# load_model function has been moved to positronic_brain/model_io.py


# move_cache_to_device function has been moved to positronic_brain/model_io.py


# truncate_kv_cache function has been moved to positronic_brain/model_io.py


# SimpleContextController has been moved to positronic_brain/controller.py


# vram_monitor_task has been moved to positronic_brain/vram_monitor.py


# _async_save_context_text has been moved to positronic_brain/utils.py

async def _handle_context_update(
    model: AutoModelForCausalLM,
    processor: AutoProcessor,
    kv_mirror_manager: KVMirror,
    input_ids: torch.Tensor,
    attention_mask: torch.Tensor,
    past_key_values: Optional[tuple],
    shared_state: Dict,
    output_queue: asyncio.Queue,
    update_tokens: torch.Tensor,
    update_attention_mask: torch.Tensor,
    update_token_ids: list,
    kv_patcher: KVCachePatcher = None,
    pending_diffs_queue: asyncio.Queue = None
) -> tuple[torch.Tensor, torch.Tensor, Optional[tuple], bool]:
    """
    Handles the logic when a context update (injection) is processed.
    Performs incremental forward pass, registers tokens, and handles pruning if needed.
    Returns updated input_ids, attention_mask, past_key_values, and a boolean indicating if an update occurred.
    """
    try:
        print("[Inference] Performing incremental update for injected context", file=sys.stderr)
        
        # Use only the new tokens as input for this forward pass
        model_input_ids = update_tokens.to(model.device)
        
        # Store full mask for state update, but don't pass to model call
        current_attention_mask_for_call = torch.cat([attention_mask, update_attention_mask], dim=1).to(model.device)
        
        # Log shapes for debugging
        print(f"[Debug] Incremental update shapes: input_ids={model_input_ids.shape}, "  
              f"attention_mask would be {current_attention_mask_for_call.shape}, "  
              f"existing tokens={input_ids.shape[1]}, new tokens={update_tokens.shape[1]}", file=sys.stderr)
        
        # Prepare proper attention mask for context injection
        # For Kimi-VL models, we need to ensure the mask matches expected shape
        if past_key_values is not None:
            # Get the current KV cache sequence length
            cache_seq_len = past_key_values[0][0].shape[2] if past_key_values else 0
            
            # Get the device of the first cache tensor
            target_device = past_key_values[0][0].device if past_key_values else model.device
            
            # FIXED: For context injection with KV cache, we need to use the proper mask width
            # but ALSO create explicit position IDs to ensure proper RoPE functionality
            
            # Create position_ids for all tokens being injected
            # These should start at cache_seq_len and increment for each token
            position_ids = torch.arange(
                cache_seq_len,
                cache_seq_len + model_input_ids.shape[1],
                dtype=torch.long,
                device=target_device
            ).unsqueeze(0)  # Add batch dimension
            
            # Create a boolean attention mask for injected tokens
            # Use the simple attention mask from update_attention_mask
            injection_attention_mask = update_attention_mask
            
            # Debug logging
            print(f"[Injection] Creating position_ids from {cache_seq_len} to {cache_seq_len + model_input_ids.shape[1] - 1}")
            print(f"[Injection] Using attention mask of shape {injection_attention_mask.shape} for input_ids {model_input_ids.shape}")
        else:
            # No cache yet, use None for injection_attention_mask and no explicit position_ids
            injection_attention_mask = None
            position_ids = None
        
        # --- Apply Pending Patches (if any) ---
        if kv_patcher is not None and pending_diffs_queue is not None and past_key_values is not None:
            diffs_to_apply = []
            while not pending_diffs_queue.empty():
                try:
                    # Get all currently available diffs without blocking indefinitely
                    diff_item = pending_diffs_queue.get_nowait()
                    diffs_to_apply.append(diff_item)
                    pending_diffs_queue.task_done()  # Mark task done for queue management
                except asyncio.QueueEmpty:
                    break  # No more items currently available

            if diffs_to_apply:
                print(f"[Patcher Hook] Applying {len(diffs_to_apply)} patches before context update forward pass...")
                
                # 1. Apply diffs to the KVMirror state FIRST
                #    This updates the canonical map of which token *should* be at each position.
                try:
                    update_summary = kv_mirror_manager.apply_diff(diffs_to_apply)
                    print(f"[KVMirror ApplyDiff] Summary: {update_summary}")
                    # Handle potential failures reported in update_summary if necessary
                except Exception as mirror_e:
                    print(f"[Error] Failed applying diff to KVMirror: {mirror_e}")
                    # Decide how to proceed - skip patching cache? Log error?
                    # For now, log and potentially skip cache patch for this cycle.
                    diffs_to_apply = [] # Clear diffs if mirror update failed

                # 2. Apply diffs to the KV Cache Tensors (if mirror update succeeded)
                if diffs_to_apply and past_key_values is not None:
                    try:
                        # Patch KV cache using our architecture-aware patcher implementation
                        past_key_values = kv_patcher.patch(past_key_values, diffs_to_apply)
                        print(f"[KVPatcher] KV cache patching attempted.")
                    except Exception as patch_e:
                        print(f"[Error] KVCachePatcher failed: {patch_e}")
                        # If patching fails, we might have inconsistent state.
                        # Safest might be to use the unpatched cache? Or log severity?
                        # For now, log the error. past_key_values remains unpatched by patcher.
        # --------------------------
        
        # Perform an incremental forward pass with the existing KV cache using external function
        try:
            # Use torch.no_grad() for inference optimization
            with torch.no_grad():
                # Include position_ids when using KV cache
                if past_key_values is not None and position_ids is not None:
                    logits, llm_output_past_key_values, outputs_attentions = execute_forward_pass(
                        model=model,
                        input_ids=model_input_ids,
                        attention_mask=injection_attention_mask,  # Use our correctly sized mask
                        position_ids=position_ids,  # Explicitly pass position_ids 
                        past_key_values=past_key_values  # Use existing cache
                    )
                else:
                    # Initial pass or no explicit position_ids needed
                    logits, llm_output_past_key_values, outputs_attentions = execute_forward_pass(
                        model=model,
                        input_ids=model_input_ids,
                        attention_mask=injection_attention_mask,  # Use our correctly sized mask
                        past_key_values=past_key_values  # Use existing cache
                    )
            
            # Prepare a minimal outputs object for pruning function compatibility
            outputs = type('ModelOutputs', (), {})()
            outputs.attentions = outputs_attentions
            outputs.past_key_values = llm_output_past_key_values
            outputs.logits = logits
        except Exception as model_e:
            # Error handling for model execution is handled at a higher level
            print(f"[Error] Forward pass failed during context update: {str(model_e)}")
            raise model_e
        
        # Register injected tokens in the KV Mirror structure
        current_position = kv_mirror_manager.get_current_size()
        for i, token_id in enumerate(update_token_ids):
            # Register in the KV Mirror using atomic function
            position = current_position + i
            instance_id = kv_mirror_manager.add(
                token_id=token_id,
                position=position,
                source='user_inject',
                removal_bias=0.2  # Give injected tokens a slight bias to be kept
            )
            print(f"[KV Mirror] Injected token {token_id} registered at position {position} with ID {instance_id}")
        
        # Update the main context state variables
        input_ids = torch.cat([input_ids, update_tokens], dim=1) 
        attention_mask = current_attention_mask_for_call  # Still update the full mask for state tracking
        
        # Context management for injected tokens - using the Compactor/Brightness approach
        # The KV Mirror state is already updated above with the new tokens
        # For now, we'll just perform basic logging and continue without pruning
        if llm_output_past_key_values is not None:
            # Get the current cache length
            current_cache_len = llm_output_past_key_values[0][0].shape[2]
            
            # Log information about context size
            print(f"[Context Injection] Current context length: {current_cache_len} tokens", file=sys.stderr)
            
            # Temporarily disable brightness score initialization for O1 triage testing
            # for i in range(update_tokens.shape[1]):
            #     # Get the position in the overall context
            #     token_pos = current_cache_len - update_tokens.shape[1] + i
            #     try:
            #         # Set initial brightness for new tokens
            #         # We use a neutral initial value since we don't have attention scores yet
            #         kv_mirror_manager.set_token_brightness(token_pos, config.INITIAL_TOKEN_BRIGHTNESS)
            #         # Token brightness will be properly updated in _generate_next_token after the next forward pass
            #     except Exception as e:
            #         print(f"[Brightness] Failed to initialize brightness for token at position {token_pos}: {e}", file=sys.stderr)
            print(f"[O1 Triage] Brightness initialization disabled for testing", file=sys.stderr)
            
            # Get statistics for logging
            stats = kv_mirror_manager.get_stats()
            active_tokens = stats['active_tokens']
            total_tracked = stats['total_tokens']
            
            print(f"[KV Mirror] After injection: {active_tokens} active tokens, {total_tracked} total tracked")
            
            # Update the shared state for UI if needed
            if shared_state is not None:
                async with shared_state['lock']:
                    shared_state['kv_mirror_stats'] = {
                        'active': active_tokens,
                        'total': total_tracked,
                        'timestamp': time.time()
                    }
            
            # Simply use the past_key_values directly - no pruning/manipulation needed
            past_key_values = llm_output_past_key_values
            # The tokens have already been registered above via register_token
            # No further updates needed here
        else:
            past_key_values = llm_output_past_key_values
        
        print(f"[Inference] Incremental update complete. Context length: {input_ids.shape[1]}", file=sys.stderr)
        return input_ids, attention_mask, past_key_values, True
    except Exception as e:
        error_msg = f"\n\n[ERROR] Context injection error: {type(e).__name__}: {str(e)}\n"
        print(error_msg, file=sys.stderr)
        print(f"[Fallback] Resetting KV cache due to incremental update failure", file=sys.stderr)
        await output_queue.put(error_msg)
        
        # Fallback strategy: Reset KV cache and append new tokens to input context anyway
        past_key_values = None  # Reset cache to force full recomputation next iteration
        input_ids = torch.cat([input_ids, update_tokens], dim=1)  # Still add the new tokens
        attention_mask = torch.cat([attention_mask, update_attention_mask], dim=1)  # Update mask
        
        # Return the updated state after fallback
        return input_ids, attention_mask, past_key_values, True

async def _generate_next_token(
    model: AutoModelForCausalLM,
    processor: AutoProcessor,
    kv_mirror_manager: KVMirror,
    input_ids: torch.Tensor,
    attention_mask: torch.Tensor,
    past_key_values: Optional[tuple],
    sampler_state: SamplerState,
    shared_state: Dict,
    output_queue: asyncio.Queue,
    resume_generation_event: asyncio.Event,
    stop_tokens: set[int],
    pause_tokens: set[int],
    kv_patcher: KVCachePatcher = None,
    pending_diffs_queue: asyncio.Queue = None
) -> tuple[torch.Tensor, torch.Tensor, Optional[tuple]]:
    """
    Handles the standard token generation path.
    Performs forward pass, sampling, token registration, output queuing, and pruning if needed.
    Returns updated input_ids, attention_mask, past_key_values.
    """
    try:
        # DISABLED: Optimize GPU memory using move_cache_to_device
        # if config.OFFLOAD_KV_CACHE_TO_CPU and past_key_values is not None:
        #     past_key_values = move_cache_to_device(past_key_values, model.device)

        # Prepare inputs for standard token generation
        if past_key_values is None:
            # Initial pass without KV cache - need full attention mask
            model_input_ids = input_ids
            current_attention_mask_for_call = attention_mask
        else:
            # Using KV cache - only need the last token with a matching attention mask
            model_input_ids = input_ids[:, -1:]  # Only need the last token with KV cache
            
            # For Kimi-VL model, we need to create an attention mask that matches the expected shape
            # The error "Attention mask should be of size (1, 1, 1, 501), but is torch.Size([1, 1, 1, 500])"
            # indicates we need to account for both the new token and the KV cache length
            
            # Get the current KV cache sequence length
            cache_seq_len = past_key_values[0][0].shape[2] if past_key_values else 0
            
            # Using the simplest approach: create a new attention mask of ones with correct size
            # Most models expect attention_mask in shape [batch_size, seq_length]
            # But Kimi-VL might expect a different shape based on the error message (1, 1, 1, N)
            # This covers both formats by starting with [batch_size, seq_length] and letting the model reshape if needed
            
            # First, get the device of the first cache tensor to ensure proper device placement
            target_device = past_key_values[0][0].device
            
            # O1 Fix: Set attention_mask to None during KV cache steps
            # Let HF handle the internal causal mask construction
            # This prevents confusing the internal mask preparation (_prepare_decoder_attention_mask)
            current_attention_mask_for_call = None
            
            # FIXED: Explicitly create position_ids to ensure correct positioning with KV cache
            # This is critical for RoPE to work correctly
            position_ids = torch.full(
                (1, 1),  # Shape for one new token
                cache_seq_len,  # The next absolute position
                device=model_input_ids.device,  # Match input device
                dtype=torch.long
            )

        try:
            # Debug logging for standard generation path
            print(f"[Debug] Standard generation shapes: input_ids={model_input_ids.shape}, " 
                  f"attention_mask={current_attention_mask_for_call.shape if current_attention_mask_for_call is not None else 'None'}, " 
                  f"using_kv_cache={past_key_values is not None}, " 
                  f"context_length={input_ids.shape[1]}", file=sys.stderr)
        except Exception as e:
            print(f"[Debug] Error logging standard generation shapes: {e}", file=sys.stderr)
            
        # --- Step 1: Standard Model Forward Pass for Token Generation ---
        # Always use attention mask for Kimi-VL model (it has an assert attention_mask is not None)
        
        # O1 Fix: Removed defensive check for attention mask dimensions
        # When attention_mask is None, we can't check its shape
        # if past_key_values is not None and current_attention_mask_for_call is not None:
        #     current_input_len = model_input_ids.shape[1]  # Should be 1 if using cache
        #     cache_len = past_key_values[0][0].shape[2]
        #     # The model expects input len + cache len for attention mask
        #     expected_attn_len = current_input_len + cache_len
        #     if current_attention_mask_for_call.shape[1] != expected_attn_len:
        #         print(f"[ASSERT FAIL] Attention mask length mismatch! Mask: {current_attention_mask_for_call.shape[1]}, Expected: {expected_attn_len} (Input: {current_input_len}, Cache: {cache_len})", file=sys.stderr)
        
        # --- Apply Pending Patches (if any) ---
        if kv_patcher is not None and pending_diffs_queue is not None and past_key_values is not None:
            diffs_to_apply = []
            while not pending_diffs_queue.empty():
                try:
                    # Get all currently available diffs without blocking indefinitely
                    diff_item = pending_diffs_queue.get_nowait()
                    diffs_to_apply.append(diff_item)
                    pending_diffs_queue.task_done()  # Mark task done for queue management
                except asyncio.QueueEmpty:
                    break  # No more items currently available

            if diffs_to_apply:
                print(f"[Patcher Hook] Applying {len(diffs_to_apply)} patches before token generation forward pass...")
                
                # 1. Apply diffs to the KVMirror state FIRST
                #    This updates the canonical map of which token *should* be at each position.
                try:
                    update_summary = kv_mirror_manager.apply_diff(diffs_to_apply)
                    print(f"[KVMirror ApplyDiff] Summary: {update_summary}")
                    # Handle potential failures reported in update_summary if necessary
                except Exception as mirror_e:
                    print(f"[Error] Failed applying diff to KVMirror: {mirror_e}")
                    # Decide how to proceed - skip patching cache? Log error?
                    # For now, log and potentially skip cache patch for this cycle.
                    diffs_to_apply = [] # Clear diffs if mirror update failed

                # 2. Apply diffs to the KV Cache Tensors (if mirror update succeeded)
                if diffs_to_apply and past_key_values is not None:
                    try:
                        # Patch KV cache using our architecture-aware patcher implementation
                        past_key_values = kv_patcher.patch(past_key_values, diffs_to_apply)
                        print(f"[KVPatcher] KV cache patching attempted.")
                    except Exception as patch_e:
                        print(f"[Error] KVCachePatcher failed: {patch_e}")
                        # If patching fails, we might have inconsistent state.
                        # Safest might be to use the unpatched cache? Or log severity?
                        # For now, log the error. past_key_values remains unpatched by patcher.
        # --------------------------
        
        # --- Step 1: Standard Model Forward Pass using external function ---
        try:
            # Use torch.no_grad() for inference optimization
            with torch.no_grad():
                # Include position_ids when using KV cache
                if past_key_values is not None:
                    # O1 Fix: Handle None attention mask correctly
                    # Don't call .to() on None - just pass None directly to execute_forward_pass
                    logits, llm_output_past_key_values, outputs_attentions = execute_forward_pass(
                        model=model, 
                        input_ids=model_input_ids.to(model.device),
                        attention_mask=current_attention_mask_for_call,  # Could be None, which is what we want
                        position_ids=position_ids,  # Explicitly pass position_ids
                        past_key_values=past_key_values
                    )
                else:
                    # Initial pass (no KV cache) doesn't need explicit position_ids
                    # But we still need to handle None attention masks correctly
                    logits, llm_output_past_key_values, outputs_attentions = execute_forward_pass(
                        model=model, 
                        input_ids=model_input_ids.to(model.device),
                        attention_mask=None if current_attention_mask_for_call is None else current_attention_mask_for_call.to(model.device),
                        past_key_values=past_key_values
                    )
            # Update past_key_values for the next iteration
            past_key_values = llm_output_past_key_values
            
            # Prepare a minimal outputs object for pruning function compatibility
            outputs = type('ModelOutputs', (), {})()
            outputs.attentions = outputs_attentions
            
            # DISABLED: Brightness update integration and token repair
            # print("Brightness updates and token repair disabled for testing")
                
            outputs.past_key_values = llm_output_past_key_values
            outputs.logits = logits
        except Exception as model_e:
            # Error handling for model execution is handled at a higher level
            raise model_e

        
        # DISABLED: Move KV cache to CPU if configured (helps with VRAM management)
        # if config.OFFLOAD_KV_CACHE_TO_CPU and past_key_values is not None:
        #     past_key_values = move_cache_to_device(past_key_values, config.CPU_DEVICE)

        # --- Step 2: Select Next Token (Sampling) using external function ---
        # Call the dedicated sampling function
        selected_token_id, final_probs, top_token_data_for_ui = select_next_token(
            logits=logits,
            input_ids=input_ids, # Pass current input_ids
            sampler_state=sampler_state
        )
        
        # Process tokens for UI display (decode token IDs to readable text)
        processed_top_tokens = []
        if processor:
            for token_info in top_token_data_for_ui:
                token_id = token_info['token_id']
                try:
                    decoded_token = processor.decode([token_id])
                    processed_top_tokens.append({
                        'token': decoded_token,
                        'token_id': token_id,
                        'probability': token_info['probability']
                        # Add bias info here if needed
                        # 'bias': sampler_state.token_bias.get(token_id, 0.0)
                    })
                except Exception as decode_err:
                    print(f"[Sampler UI] Error decoding token ID {token_id}: {decode_err}")
        
        # Update the tokens list using the dedicated utility function
        await update_top_tokens(processed_top_tokens)
        
        # The selected token ID is now directly available
        generated_token_id = selected_token_id
        
        # --- Step 3: Decode Token ---
        next_token_text = processor.decode([generated_token_id])
        
        # Create a tensor version for adding to input_ids later
        next_token = torch.tensor([[generated_token_id]], device=input_ids.device) # Match device
        
        # Check if we need to handle stop or pause tokens
        skip_iteration = False
        
        if generated_token_id in stop_tokens:
            print(f"[Generation] Stop token detected (ID: {generated_token_id}). Pausing generation.")
            await output_queue.put("<<AI_TURN_ENDED>>")
            
            # Clear the resume event and wait for it to be set
            resume_generation_event.clear()
            print("[Generation] Waiting for input to resume generation...")
            await resume_generation_event.wait()
            print("[Generation] Generation resuming after stop token")
            skip_iteration = True
            
        elif generated_token_id in pause_tokens:
            print(f"[Generation] Pause token detected (ID: {generated_token_id}). Pausing generation temporarily.")
            await output_queue.put("<<AI_PAUSED>>")
            
            # Clear the resume event and wait with timeout
            resume_generation_event.clear()
            print("[Generation] Waiting for input or timeout (30 seconds)...")
            try:
                await asyncio.wait_for(resume_generation_event.wait(), timeout=30.0)
                print("[Generation] Generation resuming after pause token (input received)")
                skip_iteration = True
            except asyncio.TimeoutError:
                print("[Generation] Pause timeout expired, continuing generation")
                await asyncio.sleep(0.5)  # Brief pause before continuing
        
        # If we're not skipping this iteration, register the token and update state
        if not skip_iteration:
            # Update input context with the new token
            input_ids = torch.cat([input_ids, next_token], dim=1)
            
            # Update attention mask (adding a 1 for the new token)
            token_attention = torch.ones((1, 1), device=attention_mask.device)
            attention_mask = torch.cat([attention_mask, token_attention], dim=1)
            
            # Initialize shared state variables if they don't exist
            if shared_state is not None:
                async with shared_state['lock']:
                    if 'context_buffer' not in shared_state:
                        shared_state['context_buffer'] = []
                    if 'token_frequency' not in shared_state:
                        shared_state['token_frequency'] = {}
                    
            # Register the new token in the KV Mirror structure
            new_position = kv_mirror_manager.get_current_size()  # This will be its position after appending
            
            # Register in the KV Mirror using the manager's add method
            instance_id = kv_mirror_manager.add(
                token_id=generated_token_id,
                position=new_position,
                source='generated',
                removal_bias=0.0  # Default bias, could be customized based on token importance
            )
            
            # Update shared state if present
            if shared_state is not None:
                async with shared_state['lock']:
                    shared_state['input_len'] = input_ids.shape[1]
                    shared_state['last_token'] = generated_token_id
            
            # Add to the context buffer in shared state (for prompt reconstruction later if needed)
            if shared_state is not None and 'context_buffer' in shared_state:
                async with shared_state['lock']:
                    shared_state['context_buffer'].append(next_token_text)
                    
            # Track token info for downstream use in shared state
            if shared_state is not None and 'token_frequency' in shared_state:
                async with shared_state['lock']:
                    if generated_token_id not in shared_state['token_frequency']:
                        shared_state['token_frequency'][generated_token_id] = 0
                    shared_state['token_frequency'][generated_token_id] += 1
            
            # Queue the output token for display
            await output_queue.put(next_token_text)
            
            # --- SYNCHRONOUS MAINTENANCE PHASE ---
            # Run the maintenance phase immediately after token generation
            # This performs brightness updates, culling, and repair in-sequence
            try:
                from positronic_brain.context_maintenance import ContextMaintenance
                
                # Create maintenance handler if not already available
                global maintenance_handler
                if not globals().get('maintenance_handler'):
                    # Initialize global ContextMaintenance instance
                    from positronic_brain.diffuser_runner import DiffuserModel
                    
                    global diffuser_model
                    maintenance_handler = ContextMaintenance(
                        kv_mirror_manager=kv_mirror_manager,
                        diffuser=diffuser_model,
                        kv_patcher=kv_patcher,
                        main_model=model,
                        processor=processor
                    )
                    print(f"[Maintenance] Initialized ContextMaintenance handler", file=sys.stderr)
                
                # Run maintenance phase synchronously, including:
                # 1. Brightness updates
                # 2. Culling based on brightness
                # 3. Token repair (diffusion) if needed
                patched_past_key_values, maintenance_events = await maintenance_handler.run_phase(
                    model_outputs=outputs,
                    current_input_ids=input_ids,
                    current_attention_mask=attention_mask,
                    current_past_key_values=past_key_values,
                    generation_step=shared_state.get('total_tokens_generated', new_position)
                )
                
                # Use patched KV cache if maintenance modified it
                if patched_past_key_values is not None:
                    past_key_values = patched_past_key_values
                    print(f"[Maintenance] Using patched KV cache", file=sys.stderr)
                
                # Log maintenance events
                if maintenance_events:
                    for event in maintenance_events:
                        print(f"[Maintenance] Event: {event['type']}", file=sys.stderr)
                        
                        # Update shared state with maintenance metrics
                        if shared_state is not None and 'maintenance_metrics' in shared_state:
                            async with shared_state['lock']:
                                if 'events' not in shared_state['maintenance_metrics']:
                                    shared_state['maintenance_metrics']['events'] = []
                                shared_state['maintenance_metrics']['events'].append(event)
            except Exception as e:
                print(f"[Maintenance] Error during maintenance phase: {type(e).__name__}: {str(e)}", file=sys.stderr)
                import traceback
                traceback.print_exc()
                # Continue without stopping token generation
        
        return input_ids, attention_mask, past_key_values
    except Exception as e:
        print(f"[Generation] Error in token generation: {type(e).__name__}: {str(e)}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        # Re-raise to let the main loop handle the error
        raise

async def run_continuous_inference(model, processor, controller, initial_prompt_content=None, output_queue=None, shutdown_event=None, sliding_event=None, resume_context_file=None, shared_state=None, kv_patcher=None, pending_diffs_queue=None, compactor_request_queue=None):
    """Runs continuous inference, pushing output tokens to the queue for consumers
    
    Args:
        model: The LLM
        processor: The processor for the model
        controller: The SimpleContextController for injections
        initial_prompt_content: Optional initial prompt
        output_queue: Queue where tokens are pushed
        shutdown_event: Event to signal shutdown
        sliding_event: Event to signal when sliding window should be applied
        resume_context_file: Optional file to load context from
        shared_state: Shared state dictionary
        kv_patcher: KV Cache Patcher for applying diffs to KV cache
    """
    global kv_mirror_manager, dynamic_ceiling
    
    # Default values for backward compatibility
    if output_queue is None:
        output_queue = asyncio.Queue()
    # Create default shared state if not provided
    if shared_state is None:
        shared_state = {}

    # Create resume_generation_event for controlling generation resumption
    resume_generation_event = asyncio.Event()
    resume_generation_event.set()  # Start with generation enabled
    
    # Store the event in shared state for external access
    if shared_state is not None:
        async with shared_state['lock']:
            shared_state['resume_generation_event'] = resume_generation_event
    
    # Initialize KV Mirror for tracking token context if not already initialized
    global kv_mirror_manager
    if kv_mirror_manager is None:
        print(f"[KVMirror] Initializing KV Mirror...")
        kv_mirror_manager = KVMirror()
        print(f"[KVMirror] Initialized successfully")
    else:
        # Clear the KV Mirror to reset state
        kv_mirror_manager.clear()
    
    # Initialize the diffuser model for token repair if not already initialized
    global diffuser_model
    if diffuser_model is None:
        try:
            print(f"[Diffuser] Loading diffuser model {config.DIFFUSER_MODEL_NAME}...")
            diffuser_model = DiffuserModel(model_name=config.DIFFUSER_MODEL_NAME)
            print(f"[Diffuser] Model loaded successfully")
        except Exception as e:
            print(f"[Diffuser] Error loading diffuser model: {e}")
            diffuser_model = None
        
    # Initialize the KV Cache Patcher if not already initialized
    if kv_patcher is None:
        try:
            print(f"[KVPatcher] Initializing KV Cache Patcher...")
            kv_patcher = KVCachePatcher(model)
            print(f"[KVPatcher] Initialized successfully")
        except Exception as e:
            print(f"[KVPatcher] Error initializing KV Cache Patcher: {e}")
            kv_patcher = None
    
    # VRAM monitoring disabled - no longer using sliding window approach
    # sliding_event is just initialized but not actively used
    print("[VRAM] Monitoring disabled - using fixed window size")
    
    # Lookup token IDs for stop and pause detection
    try:
        # Look for the primary end-of-turn token used by the model
        im_end_token_id = None
        try:
            # Try to get the token ID for 
            im_end_token_id = processor.convert_tokens_to_ids("")
            if im_end_token_id == processor.unk_token_id:
                im_end_token_id = None
                print("[Token Lookup]  token not found in vocabulary, falling back to eos_token")
            else:
                print(f"[Token Lookup] Found  token ID: {im_end_token_id}")
        except:
            print("[Token Lookup] Error looking up  token, falling back to eos_token")
        
        # Get standard EOS token ID
        eos_token_id = processor.eos_token_id
        print(f"[Token Lookup] Using eos_token_id: {eos_token_id}")
        
        # Try to find pause token if model supports it
        im_stop_token_id = None
        try:
            # Try to get the token ID for <|im_stop|>
            im_stop_token_id = processor.convert_tokens_to_ids("<|im_stop|>")
            if im_stop_token_id == processor.unk_token_id:
                im_stop_token_id = None
                print("[Token Lookup] <|im_stop|> token not found in vocabulary")
            else:
                print(f"[Token Lookup] Found <|im_stop|> token ID: {im_stop_token_id}")
        except:
            print("[Token Lookup] Error looking up <|im_stop|> token")
        
        # Define sets for stop and pause tokens
        stop_tokens = {eos_token_id}
        if im_end_token_id is not None:
            stop_tokens.add(im_end_token_id)
        
        pause_tokens = set()
        if im_stop_token_id is not None:
            pause_tokens.add(im_stop_token_id)
            
        print(f"[Token Lookup] Stop tokens: {stop_tokens}, Pause tokens: {pause_tokens}")
    except Exception as e:
        print(f"[Token Lookup] Error setting up token detection: {e}")
        stop_tokens = {processor.eos_token_id}
        pause_tokens = set()
    try:
        conf = model.config
        max_length = getattr(conf, 'seq_length', None) or getattr(conf, 'max_position_embeddings', 131072)
        print(f"Model configured max sequence length: {max_length}")
        dynamic_ceiling = max_length - 512
        print(f"--- Sliding window will target size: {dynamic_ceiling} once VRAM threshold is hit ---", flush=True)
    except Exception:
        print("Warning: Could not determine max length accurately. Using 128k.", file=sys.stderr)
        max_length = 131072
        dynamic_ceiling = max_length - 512
        print(f"--- Sliding window will target fallback size: {dynamic_ceiling} once VRAM threshold is hit ---", flush=True)
    # Original large window_size is no longer the primary trigger
    # window_size = min(max_length - 512, 100000)
    if config.OFFLOAD_KV_CACHE_TO_CPU:
        print("--- KV Cache configured for CPU offload (if needed) ---", flush=True)

    # --- Check if initial_prompt_content is already formatted or needs formatting ---
    if initial_prompt_content is None:
        initial_prompt_content = "The simulation awakens. A stream of consciousness begins to flow. What thoughts emerge?"
    
    print(f"[Startup] Received initial_prompt_content (first 100 chars): {initial_prompt_content[:100]}...")
    
    # Check if the received content looks like pre-formatted chat history
    # Look for common chat template markers like <|im_system|> or 
    if initial_prompt_content and ("<|im_system|>" in initial_prompt_content or 
                                "" in initial_prompt_content or
                                "<|assistant|>" in initial_prompt_content):
        print("[Startup] Detected pre-formatted resume context. Tokenizing directly.")
        # Tokenize the loaded text directly, assuming it's already formatted
        prompt_inputs = processor(
            initial_prompt_content,
            return_tensors="pt",
            add_special_tokens=False  # Don't add BOS/EOS again
        ).to(model.device)
        
        input_ids = prompt_inputs["input_ids"]
        # Create attention mask matching the loaded input_ids
        attention_mask = torch.ones_like(input_ids).to(model.device)
    else:
        print("[Startup] Using default or unformatted prompt. Applying chat template.")
        # Format as chat message for processing
        messages = [
            {
                "role": "user",
                "content": [{"type": "text", "text": initial_prompt_content}],
            },
        ]
        
        # Get the formatted text string first
        prompt_text_formatted = processor.apply_chat_template(
            messages, add_generation_prompt=True, tokenize=False
        )
        
        # Tokenize the formatted string using the processor (handles multimodal if needed)
        prompt_inputs = processor(text=prompt_text_formatted, images=None, return_tensors="pt").to(model.device)

        # Get input_ids and attention_mask from tokenized input
        input_ids = prompt_inputs["input_ids"]
        attention_mask = prompt_inputs.get("attention_mask", torch.ones_like(input_ids)).to(model.device)
        
    print(f"[Startup] Final initial input_ids shape: {input_ids.shape}")
    
    # Initialize the KV Mirror for tracking context
    # --- Initialize KV Mirror ---
    # Clear the KV Mirror state for this run
    kv_mirror_manager.clear()
    
    # Register ALL initial tokens in the KV Mirror structure, not limited by config.CONTEXT_WINDOW_TARGET
    initial_sequence_length = input_ids.shape[1]
    print(f"[KV Mirror] Initializing with ALL {initial_sequence_length} tokens from initial prompt")
    
    for i in range(initial_sequence_length):
        # Get the token ID at this position
        token_id = input_ids[0, i].item()
        
        # Register in the KV Mirror structure using the instance method
        kv_mirror_manager.add(token_id, i, source='system_init')
    
    # Verify initialization completed correctly
    current_snapshot = kv_mirror_manager.snapshot()
    mirror_size = len(current_snapshot['kv_mirror'])  # Use length of kv_mirror dict
    registry_size = len(current_snapshot['tokens'])   # Use length of tokens dict
    
    print(f"[KV Mirror] Initialization complete. Mirror size: {mirror_size}, Registry size: {registry_size}")
    assert mirror_size == initial_sequence_length, f"KV Mirror size mismatch after init! Expected {initial_sequence_length}, got {mirror_size}"
        
    # Set initial empty last token
    if shared_state is not None:
        async with shared_state['lock']:
            shared_state['last_token'] = ""
            # Store processor in shared_state for later use
            shared_state['processor'] = processor
    past_key_values = None
    generated_token_count = 0
    print(f"Initial prompt token length: {input_ids.shape[1]}")  # Debug print

    await output_queue.put("\n--- Starting Infinite Scroll AI Stream ---\n")

    with torch.no_grad():
        while not shutdown_event.is_set():
            # --- Step 0: Process Context Updates ---
            update_tokens, update_attention_mask, update_applied = await controller.process_pending_updates(model.device)
            
            # Extract all update token IDs for KV Mirror registration
            update_token_ids = update_tokens[0].tolist() if update_applied else []
            
            # If updates were processed, delegate to specialized helper function
            if update_applied:
                try:
                    # Handle context update using the dedicated helper function
                    input_ids, attention_mask, past_key_values, update_success = await _handle_context_update(
                        model=model,
                        processor=processor,
                        kv_mirror_manager=kv_mirror_manager,
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        past_key_values=past_key_values,
                        shared_state=shared_state,
                        output_queue=output_queue,
                        update_tokens=update_tokens,
                        update_attention_mask=update_attention_mask,
                        update_token_ids=update_token_ids,
                        kv_patcher=kv_patcher,
                        pending_diffs_queue=pending_diffs_queue
                    )
                    
                    # If update was unsuccessful (very rare), skip to next iteration
                    if not update_success:
                        # Sleep for a short time to prevent CPU spinning
                        await asyncio.sleep(0.01)  # Brief pause
                        continue
                except Exception as e:
                    error_msg = f"\n\n[ERROR] Context injection error: {type(e).__name__}: {str(e)}\n"
                    print(error_msg, file=sys.stderr)
                    print(f"[Fallback] Error in context update handling", file=sys.stderr)
                    await output_queue.put(error_msg)
                    
                    # Critical errors still cause a break
                    if isinstance(e, (RuntimeError, KeyboardInterrupt, asyncio.CancelledError)):
                        break
                    
                    await asyncio.sleep(1)  # Sleep a bit longer after an error
                    continue  # Skip to next iteration after an error
            
            # Normal token generation path (when no updates were applied)
            else:
                try:
                    # Generate the next token using the dedicated helper function
                    input_ids, attention_mask, past_key_values = await _generate_next_token(
                        model=model,
                        processor=processor,
                        kv_mirror_manager=kv_mirror_manager,
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        past_key_values=past_key_values,
                        sampler_state=sampler_state,
                        shared_state=shared_state,
                        output_queue=output_queue,
                        resume_generation_event=resume_generation_event,
                        stop_tokens=stop_tokens,
                        pause_tokens=pause_tokens,
                        kv_patcher=kv_patcher,
                        pending_diffs_queue=pending_diffs_queue
                    )

                # All the token generation logic including forward pass, sampling, and state updates
                # is now handled by the _generate_next_token helper function
                
                    # Update the count if token generation was successful
                    generated_token_count += 1
                    
                except Exception as e:
                    import traceback
                    error_msg = f"\n\n[ERROR] Token generation error: {type(e).__name__}: {str(e)}\n"
                    print(error_msg, file=sys.stderr)
                    print(f"[Traceback] {''.join(traceback.format_tb(e.__traceback__))}", file=sys.stderr)
                    await output_queue.put(error_msg)
                    
                    # If we had a KV cache, try resetting it as a fallback strategy
                    if past_key_values is not None:
                        print(f"[Fallback] Resetting KV cache due to standard generation failure", file=sys.stderr)
                        past_key_values = None  # Reset cache to force full recomputation next iteration
                    
                    # Critical errors still cause a break
                    if isinstance(e, (RuntimeError, KeyboardInterrupt, asyncio.CancelledError)):
                        break
                    
                    await asyncio.sleep(1)  # Sleep a bit longer after an error
                    continue  # Skip the sliding window check after an error
            
            # Simple monitoring of context length for logging purposes
            # Pruning is now fully handled by the _generate_next_token helper function
            current_length = input_ids.shape[1]
            if current_length > config.CONTEXT_WINDOW_TARGET * 2:  # Extreme edge case, should never happen
                print(f"[WARNING] Context has grown to {current_length} tokens, which is more than twice the target.")
                print(f"[WARNING] Attention-based pruning may have failed, but will not truncate as per design philosophy.")
            
            # Save the current context to the resume file at the end of each iteration
            if shared_state is not None and resume_context_file:
                try:
                    # We need the processor, ensure it's available
                    current_processor = shared_state.get('processor')
                    if current_processor is not None:
                        # Decode the current state of input_ids
                        # Use a CPU copy to avoid potential issues with ongoing GPU operations
                        context_to_save_text = current_processor.decode(
                            input_ids.detach().clone().cpu()[0],
                            skip_special_tokens=False  # Save potentially important special tokens too
                        )
                        # Launch save task - DO NOT AWAIT to avoid blocking
                        asyncio.create_task(
                            _async_save_context_text(context_to_save_text, resume_context_file)
                        )
                    else:
                        print("[Resume] Skipping save: Processor not found in shared_state.", file=sys.stderr)
                except Exception as e:
                    print(f"[Resume] Error preparing or launching save task at end of loop: {e}", file=sys.stderr)
            
            # --- Process Compactor Embedding Requests ---
            if compactor_request_queue and not compactor_request_queue.empty():
                processed_requests = 0
                while processed_requests < config.MAX_REQUESTS_PER_CYCLE and not compactor_request_queue.empty():
                    try:
                        start_pos, end_pos, repair_index_in_segment, original_token_id, reply_future = compactor_request_queue.get_nowait()
                        processed_requests += 1

                        # Skip if future is already completed or cancelled
                        if reply_future.done() or reply_future.cancelled():
                            compactor_request_queue.task_done()
                            continue

                        try:
                            # Get current sequence length
                            current_seq_len = input_ids.shape[1]
                            
                            # Check if positions are valid
                            if start_pos < 0 or end_pos > current_seq_len or start_pos >= end_pos:
                                raise ValueError(f"Invalid positions start={start_pos}, end={end_pos} for sequence length {current_seq_len}")
                            
                            # Use the provided start and end positions
                            start = start_pos
                            end = end_pos
                            
                            # Get embeddings from the embedding matrix
                            embeddings = model.get_input_embeddings().weight
                            
                            # Slice input_ids and get the corresponding embeddings
                            token_ids_slice = input_ids[0, start:end]
                            embedding_segment = embeddings[token_ids_slice].clone().detach().contiguous()
                            
                            # Slice attention mask
                            attn_mask_segment = attention_mask[0, start:end].clone().detach().contiguous()
                            
                            # Get brightness values for the segment
                            brightness_map_segment = torch.zeros(end - start)
                            for i in range(start, end):
                                if i in kv_mirror_manager.tokens:
                                    brightness_map_segment[i - start] = kv_mirror_manager.get_token_brightness(i)
                            
                            # Get original input IDs for the segment
                            original_input_ids_segment = input_ids[:, start:end].clone().detach()
                            
                            # Package result for the Future
                            result_data = {
                                "input_embeddings_segment": embedding_segment,
                                "attention_mask_segment": attn_mask_segment,
                                "brightness_map_segment": brightness_map_segment,
                                "original_input_ids_segment": original_input_ids_segment,
                                "repair_index_in_segment": repair_index_in_segment,
                                "original_token_id": original_token_id,
                                "global_position_start": start  # Needed to map diff index back
                            }
                            
                            # Send result back to compactor via the Future
                            reply_future.set_result(result_data)
                        except Exception as req_e:
                            print(f"[Embedding Request Error] {req_e}")
                            # Notify compactor of failure
                            if not reply_future.done():
                                reply_future.set_exception(req_e)
                        finally:
                            # Always mark task as done
                            compactor_request_queue.task_done()
                    except asyncio.QueueEmpty:
                        break  # No more requests this cycle
                    except Exception as e:
                        print(f"[Embedding Request Handler Error] Unexpected error: {e}")
                        # Continue processing other requests
                
                # Only log if we processed some requests
                if processed_requests > 0:
                    print(f"[Main Loop] Processed {processed_requests} embedding requests from Compactor")
            
            # Always yield control to other asyncio tasks at the end of each iteration
            await asyncio.sleep(0.01)
    
    # High-frequency resume mechanism has already saved the latest context during execution
    print("[Resume] Final shutdown complete. Last context state has been saved for resumption.")
    
    await output_queue.put("\n--- AI Stream Stopped ---\n")
    
    # Return the final context state
    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "token_count": generated_token_count
    }

    
# Persistence functions
# save_context and load_context have been moved to positronic_brain/persistence.py

# Helper to get initial model and controller
async def setup_ai_core(initial_prompt="The simulation awakens. A stream of consciousness begins to flow. What thoughts emerge?", 
                      context_file="context_history.txt",
                      resume_context_file="resume_context.txt",
                      context_window_target=config.CONTEXT_WINDOW_TARGET,
                      vram_threshold_percent=80.0, 
                      vram_check_interval=5):
    """Initialize and return core AI components, including sliding event."""
    global model, processor, diffuser_model, kv_patcher, sliding_event, kv_mirror_manager  # Ensure shared variables are accessible
    
    # Load model and processor if not already loaded
    model, processor = load_model(config.MODEL_NAME, config.TRUST_REMOTE_CODE)
    
    # Try to load resume context file first (high-priority)
    effective_initial_prompt = initial_prompt
    if resume_context_file and os.path.exists(resume_context_file):
        try:
            with open(resume_context_file, 'r', encoding='utf-8') as f:
                resume_text = f.read().strip()
                if resume_text:  # Only use if not empty
                    print(f"[Resume] Loading context from {resume_context_file} ({len(resume_text)} characters)")
                    effective_initial_prompt = resume_text
        except Exception as e:
            print(f"[Resume] Error loading resume context file: {e}")
            # Fall back to default initial_prompt
    else:
        print(f"[Resume] No resume file found at {resume_context_file}, using default prompt")
    
    # We're no longer using binary tensor persistence for simplicity and reliability
    initial_input_ids, initial_attention_mask = None, None
    
    # Create controller
    controller = SimpleContextController(processor)
    
    # Create communication channels
    output_queue = asyncio.Queue()
    shutdown_event = asyncio.Event()
    
    # Reset sliding event (in case of restart)
    sliding_event.clear()
    
    # Create shared state for UI integration using KV Mirror
    shared_state = {
        'last_token': '',                          # Track the most recent generated token
        'lock': asyncio.Lock(),                    # Lock for thread-safe access
        'kv_mirror_stats': {                       # Stats for KV mirror structure
            'active': 0,
            'pruned': 0,
            'total': 0,
            'timestamp': time.time()
        }
    }
    
    # Create queues for Compactor communication
    pending_diffs_queue = asyncio.Queue(maxsize=config.COMPACTOR_BUFFER_SIZE)
    compactor_request_queue = asyncio.Queue(maxsize=config.COMPACTOR_BUFFER_SIZE)
    
    # Initialize KV Mirror for tracking token context if not already initialized
    global kv_mirror_manager
    if kv_mirror_manager is None:
        print(f"[KVMirror] Initializing KV Mirror...")
        kv_mirror_manager = KVMirror()
        print(f"[KVMirror] Initialized successfully")
    else:
        # Clear the KV Mirror to reset state
        kv_mirror_manager.clear()
    
    # Initialize the diffuser model for token repair if not already initialized
    global diffuser_model
    if diffuser_model is None:
        try:
            print(f"[Diffuser] Loading diffuser model {config.DIFFUSER_MODEL_NAME}...")
            diffuser_model = DiffuserModel(model_name=config.DIFFUSER_MODEL_NAME)
            print(f"[Diffuser] Model loaded successfully")
        except Exception as e:
            print(f"[Diffuser] Error loading diffuser model: {e}")
            diffuser_model = None
        
    # Initialize the KV Cache Patcher if not already initialized
    if kv_patcher is None:
        try:
            print(f"[KVPatcher] Initializing KV Cache Patcher...")
            kv_patcher = KVCachePatcher(model)
            print(f"[KVPatcher] Initialized successfully")
        except Exception as e:
            print(f"[KVPatcher] Error initializing KV Cache Patcher: {e}")
            kv_patcher = None
    
    # VRAM monitoring disabled - no longer using sliding window approach
    # sliding_event is just initialized but not actively used
    print("[VRAM] Monitoring disabled - using fixed window size")
    
    return {
        "model": model,
        "processor": processor,
        "controller": controller,
        "kv_patcher": kv_patcher,      # Add KV Cache Patcher to returned components
        "output_queue": output_queue,
        "shutdown_event": shutdown_event,
        "sliding_event": sliding_event,  # Return the event
        "initial_prompt": initial_prompt,
        "initial_prompt_content": effective_initial_prompt,  # Use the loaded resume text or fallback prompt
        "resume_context_file": resume_context_file,  # Pass resume file path to the inference loop
        "shared_state": shared_state,   # Add shared state for UI integration
        "pending_diffs_queue": pending_diffs_queue,  # Queue for Compactor to send diffs to main loop
        "compactor_request_queue": compactor_request_queue  # Queue for Compactor to request embeddings
    }
